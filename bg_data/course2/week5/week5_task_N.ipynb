{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK1 = False\n",
    "TASK2 = False\n",
    "TASK3 = False\n",
    "TASK4 = False\n",
    "TASK5 = True\n",
    "TASK6 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Nov 19 2016 06:48:10)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description (DataFrames in parquet format)\n",
    "\n",
    "Location - `/data/sample264`\n",
    "\n",
    "Fields: `trackId`, `userId`, `timestamp`, `artistId`\n",
    "\n",
    "- `trackId` - `id` of the track\n",
    "- `userId` - `id` of the user\n",
    "- `artistId` - `id` of the artist\n",
    "- `timestamp` - `timestamp` of the moment the user starts listening to a track\n",
    "\n",
    "Location - `/data/meta`\n",
    "\n",
    "Fields: `type`, `Name`, `Artist`, `Id`\n",
    "\n",
    "- `Type` could be “track” or “artist”\n",
    "- `Name` is the title of the track if the type == “track” and the name of the musician or group if the type == “artist”.\n",
    "- `Artist` states for the creator of the track in case the type == “track” and for the name of the musician or group in case the type == “artist”.\n",
    "- `Id` - id of the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local [2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sparkSession.read.parquet(\"/data/sample264\")\n",
    "meta = sparkSession.read.parquet(\"/data/meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization could be done by next function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, sum\n",
    "\n",
    "def norm(df, key1, key2, field, n): \n",
    "    \n",
    "    window = Window.partitionBy(key1).orderBy(col(field).desc())\n",
    "        \n",
    "    topsDF = df.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= n) \\\n",
    "        .drop(col(\"row_number\")) \n",
    "        \n",
    "    tmpDF = topsDF.groupBy(col(key1)).agg(col(key1), sum(col(field)).alias(\"sum_\" + field))\n",
    "   \n",
    "    normalizedDF = topsDF.join(tmpDF, key1, \"inner\") \\\n",
    "        .withColumn(\"norm_\" + field, col(field) / col(\"sum_\" + field)) \\\n",
    "        .cache()\n",
    "\n",
    "    return normalizedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, rank\n",
    "\n",
    "# userTrack = data.groupBy(col(\"userId\"), col(\"trackId\")).count()\n",
    "\n",
    "# userTrackNorm = norm(userTrack, \"userId\", \"trackId\", \"count\", 1000) \\\n",
    "#         .withColumn(\"id\", col(\"userId\")) \\\n",
    "#         .withColumn(\"id2\", col(\"trackId\")) \\\n",
    "#         .withColumn(\"norm_count\", col(\"norm_count\") * 0.5) \\\n",
    "#         .select(col(\"id\"), col(\"id2\"), col(\"norm_count\"))     \n",
    "\n",
    "# window = Window.orderBy(col(\"norm_count\"))\n",
    "    \n",
    "# userTrackList = userTrackNorm.withColumn(\"position\", rank().over(window))\\\n",
    "#     .filter(col(\"position\") < 50)\\\n",
    "#     .orderBy(col(\"id\"), col(\"id2\"))\\\n",
    "#     .select(col(\"id\"), col(\"id2\"))\\\n",
    "#     .take(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val in userTrackList:\n",
    "#     print \"%s %s\" % val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(df, task):\n",
    "    if not task:\n",
    "        return\n",
    "\n",
    "    window = Window.orderBy(desc('norm_count'))\n",
    "\n",
    "    result = (\n",
    "        df.withColumn('position', rank().over(window))\n",
    "        .filter(col('position') < 50)\n",
    "        .orderBy(asc('id1'), asc('id2'))\n",
    "        .select(col('id1'), col('id2'))\n",
    "    )\n",
    "\n",
    "    for row in result.take(40):\n",
    "        print '{r.id1} {r.id2}'.format(r=row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Graph based Music Recommender. Task 1\n",
    "\n",
    "Build the edges of the type “track-track”. To do it you will need to count the collaborative similarity between all the tracks: if a user has listened To the tracks A and B together in THE limited time interval (equal to 7 minutes), then you should add 1 to the weight of the edge from vertex A to vertex B. For each track choose top 40 tracks similar to the initial one and normalize weights of its edges (divide the weight of each edge on a summary of weights of all edges).\n",
    "\n",
    "Sort the resulting Data Frame in ascending order by the column norm_count, take top 40 rows, select only the columns “id1”, “id2”, sort them in descending order this time first by “id1”, then by “id2” and print the columns “id1”, “id2” of the resulting dataframe.Example:\n",
    "\n",
    "```\n",
    "54719\t767867\n",
    "54719\t767866\n",
    "50787\t327676\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "_For all tasks use the same ipython notebook, each task should be the continuation of the previous._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_DELTA = 60 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data.alias('d1')\n",
    "data_2 = data.alias('d2')\n",
    "\n",
    "cond = (\n",
    "    (col('d1.userId') == col('d2.userId'))\n",
    "    & (col('d1.timestamp') < col('d2.timestamp'))\n",
    "    & (col('d1.timestamp') + TIME_DELTA >= col('d2.timestamp'))\n",
    "    & (col('d1.trackId') != col('d2.trackId'))\n",
    ")\n",
    "\n",
    "djoin = (\n",
    "#     data_1.crossJoin(data_2)\n",
    "    data_1.join(data_2, cond, 'left_outer')\n",
    "#     .filter(cond)\n",
    "    .select(col('d1.trackId').alias('id1'), col('d2.trackId').alias('id2'))\n",
    "    .dropna()\n",
    "    .groupBy(col('id1'), col('id2'))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "track_track = (\n",
    "    norm(djoin, 'id1', 'id2', 'count', 40)\n",
    "    .select(col('id1'), col('id2'), col('norm_count'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_result(track_track, TASK1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph based Music Recommender. Task 2\n",
    "\n",
    "Build the edges of the type “user-track”. Take the amount of times the track was listened by all users as the weight of the edge from the artist’s vertex to the track’s vertex. For each user take top-1000 and normalize them.\n",
    "\n",
    "Sort the resulting Data Frame in ascending order by the column norm_count, take top 40 rows, select only the columns “id1”, “id2”, sort them in descending order this time first by “id1”, then by “id2” and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "54719\t767867\n",
    "54719\t767866\n",
    "50787\t327676\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_track = (\n",
    "    norm(data.groupBy('userId', 'trackId').count(), 'userId', 'trackId', 'count', 1000)\n",
    "    .withColumn('id1', col('userId'))\n",
    "    .withColumn('id2', col('trackId'))\n",
    "    .select(col('id1'), col('id2'), col('norm_count'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_result(user_track, TASK2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph based Music Recommender. Task 3\n",
    "\n",
    "Build the edges of the type “user-artist”. Take the amount of times the user has listened to the artist’s tracks as the weight of the edge from the user’s vertex to the artist’s vertex. For each user take top-100 artists and normalize weights.\n",
    "\n",
    "Sort the resulting Data Frame in ascending order by the column norm_count, take top 40 rows, select only the columns “id1”, “id2”, sort them in descending order this time first by “id1”, then by “id2” and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "54719\t767867\n",
    "54719\t767866\n",
    "50787\t327676\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_artist = (\n",
    "    norm(data.groupBy('userId', 'artistId').count(), 'userId', 'artistId', 'count', 100)\n",
    "    .withColumn('id1', col('userId'))\n",
    "    .withColumn('id2', col('artistId'))\n",
    "    .select(col('id1'), col('id2'), col('norm_count'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 993426\n",
      "116 974937\n",
      "128 1003021\n",
      "131 983068\n",
      "195 997265\n",
      "215 991696\n",
      "235 990642\n",
      "288 1000564\n",
      "300 1003362\n",
      "321 986172\n",
      "328 967986\n",
      "333 1000416\n",
      "346 982037\n",
      "356 974846\n",
      "374 1003167\n",
      "428 993161\n",
      "431 969340\n",
      "445 970387\n",
      "488 970525\n",
      "542 969751\n",
      "612 987351\n",
      "617 970240\n",
      "649 973851\n",
      "658 973232\n",
      "662 975279\n",
      "698 995788\n",
      "708 968848\n",
      "746 972032\n",
      "747 972032\n",
      "776 997265\n",
      "784 969853\n",
      "806 995126\n",
      "811 996436\n",
      "837 989262\n",
      "901 988199\n",
      "923 977066\n",
      "934 990860\n",
      "957 991171\n",
      "989 975339\n",
      "999 968823\n"
     ]
    }
   ],
   "source": [
    "print_result(user_artist, TASK3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph based Music Recommender. Task 4\n",
    "\n",
    "Build the edges of the type “artist-track”. Take the amount of times the track HAS BEEN listened by all users as the weight of the edge from the artist’s vertex to the track’s vertex. For each artist take top-100 tracks and normalize weights.\n",
    "\n",
    "Sort the resulting Data Frame in ascending order by the column norm_count (the column with normalized weights), take top 40 rows, select only the columns “id1”, “id2”, sort them in descending order this time first by “id1”, then by “id2” and print the columns “id1”, “id2” of the resulting dataframe.\n",
    "\n",
    "Example:\n",
    "```\n",
    "54719\t767867\n",
    "54719\t767866\n",
    "50787\t327676\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_track = (\n",
    "    norm(data.groupBy('artistId', 'trackId').count(), 'artistId', 'trackId', 'count', 100)\n",
    "    .withColumn('id1', col('artistId'))\n",
    "    .withColumn('id2', col('trackId'))\n",
    "    .select(col('id1'), col('id2'), col('norm_count'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_result(artist_track, TASK4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph based Music Recommender. Task 5\n",
    "\n",
    "Construct balancing function where the edges of the type “user-track” and the edges of the type “user-artist” influence the final recommendations equally.\n",
    "\n",
    "For the user with Id 776748 find all the tracks and artists connected to him. Sort founded items first by artist then by name in ascending order, leave only columns ”Artist” and “Name” and print top-40\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Artist: Green Day 21 GunsArtist: Green Day Artist: Green DayArtist: Green Day Kill The DJArtist: Iggy Pop Artist: Iggy PopArtist: Iggy Pop Sunday\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
