{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Nov 19 2016 06:48:10)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description (DataFrames in parquet format)\n",
    "\n",
    "Location - `/data/sample264`\n",
    "\n",
    "Fields: `trackId`, `userId`, `timestamp`, `artistId`\n",
    "\n",
    "- `trackId` - `id` of the track\n",
    "- `userId` - `id` of the user\n",
    "- `artistId` - `id` of the artist\n",
    "- `timestamp` - `timestamp` of the moment the user starts listening to a track\n",
    "- `Location` - `/data/meta`\n",
    "\n",
    "Fields: `type`, `Name`, `Artist`, `Id`\n",
    "\n",
    "Type could be “track” or “artist”\n",
    "Name is the title of the track if the type == “track” and the name of the musician or group if the type == “artist”.\n",
    "Artist states for the creator of the track in case the type == “track” and for the name of the musician or group in case the type == “artist”.\n",
    "Id - id of the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local [2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sparkSession.read.parquet(\"/data/sample264\")\n",
    "meta = sparkSession.read.parquet(\"/data/meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization could be done by next function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, sum, desc, asc\n",
    "\n",
    "def norm(df, key1, key2, field, n): \n",
    "    \n",
    "    window = Window.partitionBy(key1).orderBy(col(field).desc())\n",
    "        \n",
    "    topsDF = df.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= n) \\\n",
    "        .drop(col(\"row_number\")) \n",
    "        \n",
    "    tmpDF = topsDF.groupBy(col(key1)).agg(col(key1), sum(col(field)).alias(\"sum_\" + field))\n",
    "   \n",
    "    normalizedDF = topsDF.join(tmpDF, key1, \"inner\") \\\n",
    "        .withColumn(\"norm_\" + field, col(field) / col(\"sum_\" + field)) \\\n",
    "        .cache()\n",
    "\n",
    "    return normalizedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, rank\n",
    "\n",
    "userTrack = data.groupBy(col(\"userId\"), col(\"trackId\")).count()\n",
    "\n",
    "userTrackNorm = (\n",
    "    norm(userTrack, \"userId\", \"trackId\", \"count\", 1000)\n",
    "    .withColumn(\"id\", col(\"userId\"))\n",
    "    .withColumn(\"id2\", col(\"trackId\"))\n",
    "    .withColumn(\"norm_count\", col(\"norm_count\") * 0.5)\n",
    "    .select(col(\"id\"), col(\"id2\"), col(\"norm_count\"))\n",
    ")\n",
    "\n",
    "window = Window.orderBy(col(\"norm_count\"))\n",
    "    \n",
    "userTrackList = (\n",
    "    userTrackNorm.withColumn(\"position\", rank().over(window))\n",
    "    .filter(col(\"position\") < 50)\n",
    "    .orderBy(col(\"id\"), col(\"id2\"))\n",
    "    .select(col(\"id\"), col(\"id2\"))\n",
    "    .take(40)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415763 853951\n",
      "436158 889948\n",
      "586043 800288\n",
      "586043 800317\n",
      "586043 801522\n",
      "586043 804741\n",
      "586043 805880\n",
      "586043 806233\n",
      "586043 806439\n",
      "586043 807873\n",
      "586043 808328\n",
      "586043 810571\n",
      "586043 811212\n",
      "586043 811848\n",
      "586043 815635\n",
      "586043 818116\n",
      "586043 819591\n",
      "586043 821062\n",
      "586043 822375\n",
      "586043 825775\n",
      "586043 825997\n",
      "586043 826725\n",
      "586043 831955\n",
      "586043 833018\n",
      "586043 834780\n",
      "586043 834887\n",
      "586043 835312\n",
      "586043 837744\n",
      "586043 838944\n",
      "586043 842614\n",
      "586043 844606\n",
      "586043 851992\n",
      "586043 852304\n",
      "586043 852734\n",
      "586043 852863\n",
      "586043 855577\n",
      "586043 856643\n",
      "586043 858618\n",
      "586043 858992\n",
      "586043 860220\n"
     ]
    }
   ],
   "source": [
    "for val in userTrackList:\n",
    "    print \"%s %s\" % val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Graph based Music Recommender. Task 1\n",
    "\n",
    "Build the edges of the type “track-track”. To do it you will need to count the collaborative similarity between all the tracks: if a user has listened To the tracks A and B together in THE limited time interval (equal to 7 minutes), then you should add 1 to the weight of the edge from vertex A to vertex B. For each track choose top 40 tracks similar to the initial one and normalize weights of its edges (divide the weight of each edge on a summary of weights of all edges).\n",
    "\n",
    "Sort the resulting Data Frame in ascending order by the column norm_count, take top 40 rows, select only the columns “id1”, “id2”, sort them in descending order this time first by “id1”, then by “id2” and print the columns “id1”, “id2” of the resulting dataframe.Example:\n",
    "\n",
    "```\n",
    "54719\t767867\n",
    "54719\t767866\n",
    "50787\t327676\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "_For all tasks use the same ipython notebook, each task should be the continuation of the previous._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_DELTA = 60 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808110 894437\n",
      "809289 847119\n",
      "814446 870227\n",
      "819569 800325\n",
      "827209 942995\n",
      "829292 871752\n",
      "830062 849304\n",
      "832475 925631\n",
      "832553 836728\n",
      "836522 907798\n",
      "841759 898484\n",
      "844651 897648\n",
      "844819 834559\n",
      "852427 825116\n",
      "856311 875086\n",
      "857303 943835\n",
      "875876 916850\n",
      "878289 814956\n",
      "879172 898823\n",
      "879366 814475\n",
      "882856 841509\n",
      "889636 799651\n",
      "890604 904285\n",
      "890920 838812\n",
      "895618 944759\n",
      "903281 810518\n",
      "904487 810488\n",
      "907516 845402\n",
      "923176 831580\n",
      "926952 818440\n",
      "932765 860022\n",
      "933119 883990\n",
      "935205 829417\n",
      "940165 823397\n",
      "941115 949312\n",
      "945602 955854\n",
      "946119 894774\n",
      "947056 843884\n",
      "956167 810599\n",
      "962224 919321\n"
     ]
    }
   ],
   "source": [
    "data_1 = data.alias('d1')\n",
    "data_2 = data.alias('d2')\n",
    "\n",
    "cond = (\n",
    "    (col('d1.userId') == col('d2.userId'))\n",
    "    & (col('d1.timestamp') < col('d2.timestamp'))\n",
    "    & (col('d1.timestamp') + TIME_DELTA > col('d2.timestamp'))\n",
    "    & (col('d1.trackId') != col('d2.trackId'))\n",
    ")\n",
    "\n",
    "djoin = (\n",
    "    data_1.join(data_2, cond,'left_outer')\n",
    "        .select(col('d1.trackId').alias('id1'), col('d2.trackId').alias('id2'))\n",
    "    .dropna()\n",
    "    .groupBy(col('id1'), col('id2'))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "track_track = norm(djoin, 'id1', 'id2', 'count', 40)\n",
    "\n",
    "result = (\n",
    "    track_track\n",
    "    .orderBy(desc('norm_count'))\n",
    "    .limit(40)\n",
    "    .select(col('id1'), col('id2'))\n",
    "    .orderBy(asc('id1'), asc('id2'))\n",
    ")\n",
    "\n",
    "for row in result.collect():\n",
    "    print '{r.id1} {r.id2}'.format(r=row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
